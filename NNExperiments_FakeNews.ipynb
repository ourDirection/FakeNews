{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import Bidirectional, TimeDistributed, GRU\n",
    "from keras.layers import LSTM, Input, Reshape, Concatenate, Flatten,Convolution1D\n",
    "from keras.layers import Conv1D, Conv2D, GlobalMaxPooling1D, MaxPooling1D, MaxPool2D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from os.path import expanduser, exists\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "# pd.set_option('display.max_colwidth', 300) #widen pandas rows display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "['FAKE' 'REAL']\n",
      "2 classes\n",
      "Found 105976 unique tokens.\n",
      "x_train shape: (5037, 100)\n",
      "x_test shape: (1260, 100)\n",
      "y_train shape: (5037, 2)\n",
      "y_test shape: (1260, 2)\n",
      "Found 105976 unique tokens.\n",
      "Load data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sequence_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3dd75d252e39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# x_train, y_train, x_test, y_test, vocabulary_inv = load_data_hansard(data_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Adjusting sequence length for actual size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sequence_length' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEVCAYAAAD+TqKGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+WTWqpMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdbdab3fb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('/home/paperspace/sonic/fakeNews/')\n",
    "\n",
    "from data_util import get_fake_or_real_news, get_politifact, get_fact_fake\n",
    "\n",
    "x_train, x_test, y_train, y_test, word_index, labels, num_classes, X, y = get_fake_or_real_news()\n",
    "# x_train, x_test, y_train, y_test, word_index, labels, num_classes, X, y = get_politifact()\n",
    "# x_train, x_test, y_train, y_test, word_index, labels, num_classes, X, y = get_fact_fake()\n",
    "\n",
    "vocabulary_inv = dict((v, k) for k, v in word_index.items())\n",
    "print('Found %s unique tokens.' % len(vocabulary_inv))\n",
    "vocabulary_inv[0] = \"<PAD/>\"\n",
    " \n",
    "# Data Preparation\n",
    "print(\"Load data...\")\n",
    "# x_train, y_train, x_test, y_test, vocabulary_inv = load_data(data_source)\n",
    "# x_train, y_train, x_test, y_test, vocabulary_inv = load_data_hansard(data_train)\n",
    "\n",
    "if sequence_length != x_test.shape[1]:\n",
    "    print(\"Adjusting sequence length for actual size\")\n",
    "    sequence_length = x_test.shape[1]\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))\n",
    "\n",
    "# x_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(model, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    # print(y_pred[:10])\n",
    "    # print(y_test[:10])\n",
    "    class_labels = np.argmax(y_test, axis=1) \n",
    "    # print(class_labels[:10])\n",
    "    # print(y_pred.argmax(axis=1))\n",
    "    print(metrics.classification_report(class_labels, y_pred.argmax(axis=1), \n",
    "                                        target_names=data_train['label'].unique(), digits=3))\n",
    "    \n",
    "    cm = confusion_matrix(class_labels, \n",
    "                          y_pred.argmax(axis=1))\n",
    "    \n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "#     plt.figure(figsize=(12,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "KERAS_DATASETS_DIR = expanduser('~/.keras/datasets/')\n",
    "GLOVE_FILE = 'glove.840B.300d.txt'\n",
    "\n",
    "print(\"Processing\", GLOVE_FILE)\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(KERAS_DATASETS_DIR + GLOVE_FILE, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings: %d' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_layer():\n",
    "    count = 0\n",
    "    embedding_matrix = np.random.uniform(-1,0,(len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            count += 1\n",
    "            \n",
    "    print('Word embeddings: %d' % len(embeddings_index))\n",
    "    print('found number of tokens in embedding space: ', count)\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    return embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# from keras.layers import TimeDistributed\n",
    "# from keras.layers import RepeatVector\n",
    "\n",
    "# epochs=50\n",
    "# batch_size=128\n",
    "# # configure problem\n",
    "# n_features = 50\n",
    "# n_timesteps_in = 5\n",
    "# n_timesteps_out = 2\n",
    "\n",
    "# def encoder_decoder_baseline():\n",
    "#     # define model\n",
    "#     model = Sequential()\n",
    "#     model.add(get_embedding_layer())\n",
    "#     model.add(LSTM(150, dropout=0.2, recurrent_dropout=0.2))\n",
    "# #     model.add(LSTM(150, input_shape=(n_timesteps_in, n_features)))\n",
    "#     model.add(RepeatVector(n_timesteps_in))\n",
    "#     model.add(LSTM(150, return_sequences=True))\n",
    "#     model.add(TimeDistributed(Dense(MAX_SEQUENCE_LENGTH, activation='softmax')))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "#     return model\n",
    "\n",
    "# model = encoder_decoder_baseline()\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# network_hist = model.fit(x_train, y_train,\n",
    "#                     batch_size=batch_size,\n",
    "#                     validation_data=(x_test, y_test),\n",
    "#                     epochs=epochs,\n",
    "#                     verbose=1,\n",
    "#                     validation_split=0.2)\n",
    "\n",
    "# score = model.evaluate(x_test, y_test,\n",
    "#                        batch_size=batch_size, verbose=1)\n",
    "\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])\n",
    "# print(score)\n",
    "\n",
    "# plot_history(network_hist)\n",
    "\n",
    "# # Plot normalized confusion matrix\n",
    "# plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Very hopefull... work on this\n",
    "# https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "epochs=50\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "def word_embedding_model():\n",
    "    print('building word embedding model')\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(64, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(64, 5, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    preds = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = word_embedding_model()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(score)\n",
    "print(model.metrics_names)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "# 84% 85%\n",
    "# epochs = 20\n",
    "# batch_size = 64\n",
    "\n",
    "def imbd_lstm():\n",
    "    print('Build LSTM model...')\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = imbd_lstm()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('Train...')\n",
    "network_hist = model.fit(x_train, y_train, \n",
    "                         batch_size=batch_size, epochs=epochs,\n",
    "                         verbose=1, validation_data=(x_test, y_test),\n",
    "                         validation_split=0.2)\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-RNN/\n",
    "# 81%\n",
    "# epochs=10\n",
    "batch_size=128\n",
    "\n",
    "def getRNN_classifier():\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    l_lstm = Bidirectional(LSTM(128))(embedded_sequences)\n",
    "    preds = Dense(num_classes, activation='softmax')(l_lstm)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = getRNN_classifier()\n",
    "\n",
    "    \n",
    "print(\"model fitting - Bidirectional LSTM\")\n",
    "model.summary()\n",
    "\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://github.com/Theo-/sentiment-analysis-keras-conv/blob/master/train_keras.py\n",
    "\n",
    "def sentiment_keras():\n",
    "    model = Sequential()\n",
    "\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    # Convolutional model (3x conv, flatten, 2x dense)\n",
    "    model.add(Convolution1D(64, 3, padding='same'))\n",
    "    model.add(Convolution1D(32, 3, padding='same'))\n",
    "    model.add(Convolution1D(16, 3, padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(180, activation='sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = sentiment_keras()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    epochs=epochs, verbose=1,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py\n",
    "# 81% 82%\n",
    "# set parameters:\n",
    "batch_size = 256\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "# epochs = 10\n",
    "\n",
    "def imbd_cnn():\n",
    "\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "#     model.add(get_embedding_layer())\n",
    "    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    # we use max pooling:\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "model = imbd_cnn()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('Training...')\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs, validation_split=0.2,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/master/examples/imdb_cnn_lstm.py\n",
    "# 85% - 82% -85% 86%\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "# epochs = 20\n",
    "\n",
    "def LSTMModel():\n",
    "    print('Build model...')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = LSTMModel()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('Train...')\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs, validation_split=0.2,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://github.com/bhaveshoswal/CNN-text-classification-keras/blob/master/model.py\n",
    "# 84% - %83 - 83%\n",
    "vocabulary_size = len(word_index) \n",
    "# embedding_dim = 100\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "# epochs = 20\n",
    "# batch_size = 10\n",
    "\n",
    "def CNNTextClassification():\n",
    "    # this returns a tensor\n",
    "    print(\"Creating Model...\")\n",
    "    inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    embedding = embedding_layer(inputs)\n",
    "    # Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)\n",
    "\n",
    "    reshape = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(embedding)\n",
    "\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], EMBEDDING_DIM), \n",
    "                    padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], EMBEDDING_DIM), \n",
    "                    padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], EMBEDDING_DIM), \n",
    "                    padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    output = Dense(units=num_classes, activation='softmax')(dropout)\n",
    "\n",
    "    # this creates a model that includes\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', \n",
    "                                 monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(optimizer=adam, \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = CNNTextClassification()\n",
    "print(model.summary())\n",
    "\n",
    "print(\"Traning Model...\")\n",
    "network_hist = model.fit(x_train, y_train, batch_size=batch_size, \n",
    "                         epochs=epochs, verbose=1, \n",
    "                         validation_data=(x_test, y_test))  # starts training\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras convolutional model\n",
    "# 80% 85% 85%\n",
    "\n",
    "# batch_size = 10\n",
    "epochs = 30\n",
    "\n",
    "def keras_conv():\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # model.add(Dense(256, activation='tanh'))\n",
    "    model.add(Dense(256, activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    adam = Adam(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "mdoel = keras_conv()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('Training...')\n",
    "# Fit the model\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[EarlyStopping(min_delta=0.00025, patience=2)])\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# simple \n",
    "# epochs = 20\n",
    "\n",
    "# 84%, 82%, 83%\n",
    "\n",
    "def simple_model():\n",
    "    # define model\n",
    "    print('simpel model')\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=adam, \n",
    "                  loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = simple_model()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('Training...')\n",
    "# Fit the model\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-RNN/\n",
    "# 83% 83% 86%\n",
    "epochs = 20\n",
    "\n",
    "def RNN_model():\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "\n",
    "    preds = Dense(num_classes, activation='softmax')(l_lstm)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = RNN_model()\n",
    "print(\"model fitting - Bidirectional LSTM\")\n",
    "print(model.summary())\n",
    "\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    \n",
    "    model.add(Dense(128, input_dim=MAX_SEQUENCE_LENGTH, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "#     print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "def GRUModel():\n",
    "    print('Build model...')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(GRU(lstm_output_size))\n",
    "\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = GRUModel()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "def test(model, data):\n",
    "    x_test, y_test = data\n",
    "    y_pred = model.predict(x_test, batch_size=30)\n",
    "    print('-' * 50)\n",
    "#     print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1)) / y_test.shape[0])\n",
    "#     print(y_pred)\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "#     print(y_pred)\n",
    "    \n",
    "#     print(np.argmax(y_pred, 1))\n",
    "\n",
    "#     print(classification_report(np.argmax(y_test, axis=1), \n",
    "#                                 y_pred, target_names=target_names))\n",
    "    \n",
    "#     result = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "#     print( confusion_matrix(y_test, y_pred, 1) )\n",
    "    f1 = f1_score(np.argmax(y_test, 1), y_pred, average='macro')  \n",
    "    percision = precision_score(np.argmax(y_test, 1), y_pred, average='macro')  \n",
    "    recall = recall_score(np.argmax(y_test, 1), y_pred, average='macro')\n",
    "\n",
    "    return f1, percision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size=64\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# evaluate using 10-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "models = {\n",
    "#     'not_learning' : not_learning() ,\n",
    "#     'create_model' : create_model(),\n",
    "#     'getRNN_classifier' : getRNN_classifier(),\n",
    "#     'simple_model' : simple_model(),\n",
    "#     'ker/as_conv' : keras_conv(),\n",
    "#     'RNN_model' : RNN_model(),\n",
    "#     'CNNTextClassification' : CNNTextClassification(),\n",
    "#     'LSTMModel' : LSTMModel(),\n",
    "#     'word_embedding_model' : word_embedding_model(),\n",
    "#     'sentiment_keras' : sentiment_keras(),\n",
    "#     'imbd_lstm' : imbd_lstm(),\n",
    "#     'GRUModel' : GRUModel(),\n",
    "#     'imbd_cnn' : imbd_cnn()\n",
    "}\n",
    "# def getModel(name):\n",
    "#     if(not_learning):\n",
    "#         return not_learning()\n",
    "#     else if('create_model' ):\n",
    "#         return create_model()\n",
    "#     else if('getRNN_classifier' ):\n",
    "#         return getRNN_classifier()\n",
    "#     else if('simple_model'):\n",
    "#          return simple_model()\n",
    "#     else if('keras_conv' ):\n",
    "#         return keras_conv()\n",
    "#     else if('RNN_model' ):\n",
    "#          return RNN_model(),\n",
    "#     else if('CNNTextClassification' ):\n",
    "#     #     'CNNTextClassification' : CNNTextClassification(),\n",
    "#     else if('LSTMModel'):\n",
    "#         return LSTMModel()\n",
    "#     else if('word_embedding_model' ):\n",
    "#         return word_embedding_model(),\n",
    "#     else if('sentiment_keras' ):\n",
    "#         return sentiment_keras(),\n",
    "#     else if('imbd_lstm' ):\n",
    "#          return imbd_lstm(),\n",
    "#     else if('GRUModel' ):\n",
    "#         'GRUModel' : GRUModel(),\n",
    "#         else if('create_model' ):\n",
    "# #     'imbd_cnn' : imbd_cnn()\n",
    "\n",
    "score = []\n",
    "\n",
    "\n",
    "for train_index, test_index in tqdm(skf.split(X, y)):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    model =  GRUModel() \n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs, verbose=0,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "#         result = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    f1, percision, recall = test(model=model, data=(x_test, y_test))\n",
    "    score.append({'f1': f1, 'percision' : percision, 'recall' : recall})\n",
    "    print('F1: ', f1, ' percision: ', percision, ' recall: ', recall)\n",
    "    \n",
    "#     print(score_temp)\n",
    "#     score.append([name, score_temp])\n",
    "#     print('average percision over kfolds:', np.average([x['percision'] for x in score_temp]))\n",
    "\n",
    "print('average f1 over kfolds:', np.average([x['f1'] for x in score]))\n",
    "print('average percision over kfolds:', np.average([x['percision'] for x in score]))\n",
    "print('average recall over kfolds:', np.average([x['recall'] for x in score]))\n",
    "\n",
    "# print(score)\n",
    "df_result = pd.DataFrame(score)\n",
    "df_result.head()\n",
    "\n",
    "df_result.to_csv('data/NNResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_result.plot()\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('average score over kfolds:', np.mean([x[1] for x in score]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
