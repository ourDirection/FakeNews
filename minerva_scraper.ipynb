{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing  \n",
    "\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "plt.style.use('seaborn-paper')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snopes.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "url = 'https://www.snopes.com/fact-check/page/{pageNum}/'\n",
    "\n",
    "numb_pages = 977\n",
    "story_links = []\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"Create a soup from url\"\"\"\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html_doc = urlopen(req).read()\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def get_story_links(link):\n",
    "    print('processing', link)\n",
    "    story_links = []\n",
    "    req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html_doc = urlopen(req).read()\n",
    "#     print(html_content)\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "#     story = soup.findAll('href', style=\"width=300px;\") \n",
    "    for a in soup.find_all('a', re.compile('article-link'), href=True):\n",
    "        story_links.append(a['href'])\n",
    "        \n",
    "    return story_links\n",
    "\n",
    " \n",
    "for page in range(1, numb_pages+1):\n",
    "    link = url.replace('{pageNum}', str(page))\n",
    "    story_links.append( get_story_links(link))\n",
    "\n",
    "    \n",
    "story_links =  [item for sublist in story_links for item in sublist]\n",
    "print('found number of stories', len(story_links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_link = pd.DataFrame(story_links) \n",
    "df_link.to_csv('data/snopes_links.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "def get_truth(snopes_href):\n",
    "    \"\"\"returns if the rumor is considered true or false\"\"\"\n",
    "#     snopes_url = 'http://www.snopes.com' + snopes_href\n",
    "    claim_text = ''\n",
    "    truth = 'false'\n",
    "    origin = ''\n",
    "    soup = get_soup(snopes_href)\n",
    "    review = soup.find('span', {'itemprop': 'alternateName'})\n",
    "            \n",
    "    try:\n",
    "        article_div = soup.find_all('div', class_='entry-content article-text')\n",
    "        if article_div:\n",
    "            claim_text = soup.findAll('p')[0].string.strip()     \n",
    "    except:\n",
    "        print (\"Oops!\")\n",
    "        claim_text = None\n",
    "        \n",
    "#     print(review, claim_text)\n",
    "    if review:\n",
    "        if 'rue' in review.string:\n",
    "            truth = 'True'\n",
    "    else:\n",
    "        review = soup.find('font', {'class': 'status_color'})\n",
    "        if review:\n",
    "            if 'TRUE' in review.string:\n",
    "                truth = 'True'\n",
    "                \n",
    "    return {'claim': claim_text, 'lable': truth, 'link': snopes_href}\n",
    "\n",
    "def processLink(story_link):\n",
    "#     print(story_link)\n",
    "    try:\n",
    "        return get_truth(story_link)\n",
    "    except:\n",
    "        print (\"Oops!\")\n",
    "    \n",
    "\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(num_cores)\n",
    "    \n",
    "results = Parallel(n_jobs=num_cores)(delayed(processLink)(i) for i in tqdm(story_links))\n",
    "\n",
    "# results = [item for sublist in results for item in sublist]\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('data/snopes.tsv', sep='\\t')\n",
    "   \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[:100]\n",
    "cleanedList = [x for x in results if x != None]\n",
    "df = pd.DataFrame(cleanedList)\n",
    "df.to_csv('data/snopes.tsv', sep='\\t')\n",
    "   \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "url = 'http://www.politifact.com/truth-o-meter/rulings/{category}/?page={pageNum}'\n",
    "\n",
    "categories = {'true': 110, \n",
    "              'mostly-true' : 138, \n",
    "              'half-true': 144,\n",
    "              'barely-true': 121,\n",
    "              'false' : 136,\n",
    "              'pants-fire' : 74\n",
    "             }\n",
    "\n",
    "links = []\n",
    "\n",
    "for category, pageNum in categories.items():\n",
    "     for i in range (1, pageNum + 1):\n",
    "        link = url.replace('{category}', category)\n",
    "        link = link.replace('{pageNum}', str(i))\n",
    "        links.append(link)\n",
    "        \n",
    "    \n",
    "print(links[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_statement(link):\n",
    "    print('processing', link)\n",
    "    req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html_doc = urlopen(req).read()\n",
    "#     print(html_content)\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    statements = soup.find_all(class_='statement')\n",
    "#     print(statements[10])\n",
    "    records = []\n",
    "    for statement in statements:\n",
    "        record = {}\n",
    "        record['quote'] = statement.find_all('p', class_='quote')[0].text\n",
    "        record['statement__source'] = statement.find_all('p', class_='statement__source')[0].text\n",
    "        record['statement__text'] = statement.find_all('a', class_='link')[0].text\n",
    "        record['link'] = statement.find_all('a', class_='link')[0]['href']\n",
    "        record['article__meta'] = statement.find_all('span', class_='article__meta')[0].text\n",
    "        record['ruling'] = link.split('/')[-2]\n",
    "        records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "# records = get_statement(links[1])\n",
    "# # print(records)\n",
    "# df = pd.DataFrame(records)\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def processLink(link):\n",
    "    return get_statement(link)\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(num_cores)\n",
    "    \n",
    "results = Parallel(n_jobs=num_cores)(delayed(processLink)(i) for i in links)\n",
    "\n",
    "results = [item for sublist in results for item in sublist]\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('data/politifact.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df.groupby(['ruling']).size())\n",
    "df.groupby(['ruling']).size().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenizer(text):\n",
    "    return u' '.join([w.text for w in nlp(text)])\n",
    "\n",
    "def _apply_df(args):\n",
    "    df, func, kwargs = args\n",
    "    df['statement__text'] = df['statement__text'].apply(func, **kwargs)\n",
    "    return df#df.apply(func, **kwargs)\n",
    "\n",
    "def apply_by_multiprocessing(df, func, **kwargs):\n",
    "    workers = kwargs.pop('workers')\n",
    "    pool = multiprocessing.Pool(processes=workers)\n",
    "    result = pool.map(_apply_df, [(d, func, kwargs) for d in np.array_split(df, workers)])\n",
    "    pool.close()\n",
    "    return pd.concat(list(result))\n",
    "\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(num_cores) \n",
    "\n",
    "df = pd.read_csv('data/politifact.tsv', sep='\\t')\n",
    "\n",
    "df = apply_by_multiprocessing(df, tokenizer,  workers=num_cores)\n",
    "\n",
    "df.to_csv('data/politifact.tsv', sep='\\t')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
